import os
import time
import pandas as pd
from datetime import datetime
import re
import json
import subprocess
import sys
from dotenv import load_dotenv
from openai import OpenAI
import glob

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# è¯»å–äº§å“åˆ†ææ¡†æ¶
def load_analysis_framework():
    """ä»æ–‡ä»¶åŠ è½½åˆ†ææ¡†æ¶"""
    try:
        framework_file = "analysis_framework.txt"
        if os.path.exists(framework_file):
            with open(framework_file, 'r', encoding='utf-8') as f:
                return f.read()
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ°åˆ†ææ¡†æ¶æ–‡ä»¶ {framework_file}ï¼Œå°†ä½¿ç”¨é»˜è®¤æ¡†æ¶")
            return """
## äº§å“åˆ†ææ¡†æ¶

ğŸ’¡ è¿™ä¸ªäº§å“è§£å†³çš„æ˜¯ä»€ä¹ˆé—®é¢˜ï¼Ÿ

ğŸ‘¤ ç”¨æˆ·æ˜¯è°ï¼Ÿ

ğŸ¤” ç”¨æˆ·ä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ

ğŸ—£ï¸ ç”¨æˆ·æ˜¯å¦‚ä½•è¯„ä»·å®ƒçš„ï¼Ÿ

ğŸ” å®ƒæ˜¯å¦‚ä½•æ‰¾åˆ°ç”¨æˆ·çš„ï¼Ÿ

ğŸ’° å®ƒèµšé’±å—ï¼Ÿå¤šå°‘ï¼Ÿ

ğŸ§  æˆ‘ä»è¿™ä¸ªäº§å“èº«ä¸Šå­¦åˆ°äº†ä»€ä¹ˆï¼Ÿ

ğŸ¤” å®ƒçš„ä»€ä¹ˆåšæ³•ä¸å®¹æ˜“ï¼Ÿ

ğŸ¤— ä¸€å¥è¯æ¨é”€ï¼š

ğŸ’¡ ä¸åŒçš„æ–¹æ³•ï¼š

ğŸ‰ æˆ‘èƒ½åšå‡ºæ¥å—ï¼Ÿ

ğŸ§­ å¦‚ä½•æ‰¾åˆ°ç”¨æˆ·ï¼Ÿ

ğŸ¤” ä¸ºä»€ä¹ˆæ˜¯æˆ‘ï¼Ÿ

â¤ï¸ æˆ‘èƒ½åšæŒå—ï¼Ÿ
"""
    except Exception as e:
        print(f"âŒ åŠ è½½åˆ†ææ¡†æ¶æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}ï¼Œå°†ä½¿ç”¨é»˜è®¤æ¡†æ¶")
        return """## äº§å“åˆ†ææ¡†æ¶\n\nğŸ’¡ è¿™ä¸ªäº§å“è§£å†³çš„æ˜¯ä»€ä¹ˆé—®é¢˜ï¼Ÿ\n\nğŸ‘¤ ç”¨æˆ·æ˜¯è°ï¼Ÿ\n\n..."""

# äº§å“åˆ†ææ¡†æ¶
ANALYSIS_FRAMEWORK = load_analysis_framework()

class ProductAnalysisIntegrator:
    def __init__(self, data_dir="./toolify_data", output_dir="./output", batch_size=5, timeout=120, delay=2):
        self.data_dir = data_dir
        self.output_dir = output_dir
        self.batch_size = batch_size
        self.timeout = timeout
        self.delay = delay
        self.api_key = os.getenv("DEEPSEEK_API_KEY")
        if not self.api_key:
            print("âš ï¸ æœªæ‰¾åˆ°DEEPSEEK_API_KEYç¯å¢ƒå˜é‡ï¼Œè¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®")
            self.client = None
        else:
            self.base_url = os.getenv("DEEPSEEK_API_URL", "https://api.deepseek.com/v1")
            self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        os.makedirs(data_dir, exist_ok=True)

    def find_excel_files(self, pattern=None):
        """æŸ¥æ‰¾æ‰€æœ‰éœ€è¦å¤„ç†çš„Excelæ–‡ä»¶"""
        if pattern is None:
            pattern = os.path.join(self.data_dir, "Toolify_Top_AI_Revenue_Rankings_*.xlsx")
        
        files = glob.glob(pattern)
        if not files:
            print(f"âŒ æœªæ‰¾åˆ°åŒ¹é…çš„Excelæ–‡ä»¶: {pattern}")
            print("ğŸ“¥ å°è¯•è¿è¡Œçˆ¬è™«è·å–æ•°æ®...")
            self.run_scraper()
            # é‡æ–°æ£€æŸ¥æ–‡ä»¶
            files = glob.glob(pattern)
            if not files:
                print("âš ï¸ çˆ¬è™«è¿è¡Œåä»æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶")
                return []
        
        print(f"âœ… æ‰¾åˆ° {len(files)} ä¸ªExcelæ–‡ä»¶")
        for file in files:
            print(f"   - {os.path.basename(file)}")
        
        return files
    
    def run_scraper(self):
        """è¿è¡Œçˆ¬è™«è·å–æ•°æ®"""
        scraper_path = "toolify_scraper.py"
        if not os.path.exists(scraper_path):
            print(f"âŒ æœªæ‰¾åˆ°çˆ¬è™«è„šæœ¬: {scraper_path}")
            return False
        
        try:
            print(f"ğŸ” å¼€å§‹è¿è¡Œçˆ¬è™«: {scraper_path}")
            cmd = [sys.executable, scraper_path]
            subprocess.run(cmd, check=True)
            print("âœ… çˆ¬è™«è¿è¡Œå®Œæˆ")
            return True
        except subprocess.SubprocessError as e:
            print(f"âŒ çˆ¬è™«è¿è¡Œå¤±è´¥: {str(e)}")
            return False

    def load_excel(self, file_path):
        """åŠ è½½Excelæ–‡ä»¶"""
        try:
            df = pd.read_excel(file_path)
            print(f"âœ… æˆåŠŸåŠ è½½Excelæ–‡ä»¶: {file_path} (å…± {len(df)} æ¡è®°å½•)")
            
            # æ£€æµ‹æ˜¯å¦æ˜¯ä¸­æ–‡æ–‡ä»¶
            is_chinese = False
            if "æ’å" in df.columns:
                is_chinese = True
                # æ˜ å°„ä¸­æ–‡åˆ—ååˆ°è‹±æ–‡
                column_mapping = {
                    "æ’å": "Ranking",
                    "å·¥å…·åç§°": "Tool Name",
                    "å·¥å…·é“¾æ¥": "Tool Link",
                    "ç½‘ç«™": "Website",
                    "æ”¯ä»˜å¹³å°": "Payment Platform", 
                    "æœˆè®¿é—®é‡": "Monthly Visits",
                    "æè¿°": "Description"
                }
                df = df.rename(columns=column_mapping)
            
            # æ‰“å°åˆ—åï¼Œä¾¿äºè°ƒè¯•
            print(f"ğŸ“‹ æ•°æ®æ¡†åˆ—å: {list(df.columns)}")
            
            return df, is_chinese
        
        except Exception as e:
            print(f"âŒ åŠ è½½Excelæ–‡ä»¶å¤±è´¥: {str(e)}")
            return None, False

    def analyze_product(self, product_data):
        """ä½¿ç”¨DeepSeek APIåˆ†æäº§å“"""
        try:
            # æ„å»ºåˆ†ææç¤º
            product_name = product_data.get("Tool Name", "")
            product_desc = product_data.get("Description", "")
            product_website = product_data.get("Website", "")
            product_revenue = product_data.get("Revenue", "æœªçŸ¥")
            product_visits = product_data.get("Monthly Visits", "æœªçŸ¥")

            # å¦‚æœå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨æ¨¡æ‹Ÿåˆ†æ
            if self.client is None:
                print(f"âš ï¸ APIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œç”Ÿæˆæ¨¡æ‹Ÿåˆ†æ: {product_name}")
                analysis = self.generate_mock_analysis(product_name, product_desc, product_website)
                core_value, target_users, revenue_model, pitch = self.extract_key_points(analysis)
                
                # åªè¿”å›æˆåŠŸæå–åˆ°çš„å†…å®¹
                result = {"å®Œæ•´åˆ†æ": analysis}
                
                if core_value != "æœªæå–åˆ°":
                    result["æ ¸å¿ƒä»·å€¼"] = core_value
                
                if target_users != "æœªæå–åˆ°":
                    result["ç›®æ ‡ç”¨æˆ·"] = target_users
                
                if revenue_model != "æœªæå–åˆ°":
                    result["æ”¶å…¥æ¨¡å¼"] = revenue_model
                
                if pitch != "æœªæå–åˆ°":
                    result["ä¸€å¥è¯æ¨é”€"] = pitch
                    
                return result
            
            prompt = f"""
æˆ‘éœ€è¦ä½ æ·±å…¥åˆ†æä»¥ä¸‹AIäº§å“:

äº§å“åç§°: {product_name}
äº§å“æè¿°: {product_desc}
äº§å“ç½‘ç«™: {product_website}
æœˆæ”¶å…¥: {product_revenue}
æœˆè®¿é—®é‡: {product_visits}

è¯·æŒ‰ç…§ä»¥ä¸‹æ¡†æ¶åˆ†æè¿™ä¸ªäº§å“:
{ANALYSIS_FRAMEWORK}

å›ç­”è¦é’ˆå¯¹æ€§å¼ºï¼Œæ¸…æ™°æ˜“æ‡‚ï¼Œä¸è¦æœ‰å†—ä½™å†…å®¹ã€‚ä¸éœ€è¦æ·»åŠ é¢å¤–çš„æ ‡é¢˜ï¼Œç›´æ¥å›ç­”é—®é¢˜å³å¯ã€‚
å›ç­”è¦æœ‰æ´å¯ŸåŠ›ï¼Œè¯´å‡ºäº§å“æˆåŠŸçš„å…³é”®ç‚¹ï¼Œå°¤å…¶æ˜¯å…³äºå•†ä¸šæ¨¡å¼ã€ç”¨æˆ·è·å–å’Œä»·å€¼ä¸»å¼ çš„éƒ¨åˆ†ã€‚
åŸºäºç°æœ‰ä¿¡æ¯ï¼Œå¯ä»¥é€‚å½“æ¨æµ‹ï¼Œä½†è¦åˆç†ã€‚å¦‚é‡ä¿¡æ¯ä¸è¶³ï¼Œè¯·å¦è¯šè¯´æ˜å¹¶ç»™å‡ºæœ€ä½³æ¨æµ‹ã€‚

è¯·ç»™å‡ºåˆ†æçš„åŒæ—¶ï¼Œæå–ä»¥ä¸‹å‡ ä¸ªå…³é”®ç‚¹å¹¶ç”¨ç®€çŸ­çš„ä¸€å¥è¯æ€»ç»“ï¼š
1. æ ¸å¿ƒä»·å€¼ï¼šäº§å“æœ€ä¸»è¦è§£å†³çš„é—®é¢˜
2. ç›®æ ‡ç”¨æˆ·ï¼šäº§å“é¢å‘çš„ä¸»è¦ç”¨æˆ·ç¾¤ä½“
3. æ”¶å…¥æ¨¡å¼ï¼šäº§å“ä¸»è¦çš„èµšé’±æ–¹å¼
4. ä¸€å¥è¯æ¨é”€ï¼šæœ€æœ‰åŠ›çš„æ¨å¹¿è¯­
"""

            # è°ƒç”¨API
            try:
                if self.client is None:
                    raise Exception("APIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·ç¡®ä¿æä¾›æœ‰æ•ˆçš„APIå¯†é’¥")
                
                response = self.client.chat.completions.create(
                    model="deepseek-reasoner",  # ä½¿ç”¨æ›´å¼ºå¤§çš„æ¨¡å‹
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±äº§å“ä¸“å®¶ï¼Œæ“…é•¿åˆ†æäº§å“çš„å•†ä¸šæ¨¡å¼ã€ç”¨æˆ·éœ€æ±‚å’Œå¸‚åœºç­–ç•¥"},
                        {"role": "user", "content": prompt}
                    ],
                    stream=False
                )
                
                analysis = response.choices[0].message.content
            except Exception as api_error:
                print(f"âš ï¸ APIè°ƒç”¨å¤±è´¥: {str(api_error)}ï¼Œå°†ä½¿ç”¨åˆ†ææ¡†æ¶ä½œä¸ºæç¤º")
                # å¦‚æœAPIè°ƒç”¨å¤±è´¥ï¼Œç›´æ¥è¿”å›åˆ†ææ¡†æ¶ä½œä¸ºæç¤º
                analysis = f"""## {product_name} äº§å“åˆ†æ
                
{ANALYSIS_FRAMEWORK}

(APIè°ƒç”¨å¤±è´¥ï¼Œè¯·ç›´æ¥æŒ‰ç…§ä»¥ä¸Šæ¡†æ¶åˆ†æè¯¥äº§å“)

æ ¸å¿ƒä»·å€¼ï¼šè¯·æ ¹æ®äº§å“ä¿¡æ¯åˆ†ææ ¸å¿ƒä»·å€¼
ç›®æ ‡ç”¨æˆ·ï¼šè¯·æ ¹æ®äº§å“ä¿¡æ¯åˆ†æç›®æ ‡ç”¨æˆ·
æ”¶å…¥æ¨¡å¼ï¼šè¯·æ ¹æ®äº§å“ä¿¡æ¯åˆ†ææ”¶å…¥æ¨¡å¼
ä¸€å¥è¯æ¨é”€ï¼šè¯·æ ¹æ®äº§å“ç‰¹ç‚¹æç‚¼æ¨å¹¿è¯­
"""
            
            # æå–å…³é”®åˆ†æç‚¹
            core_value, target_users, revenue_model, pitch = self.extract_key_points(analysis)
            
            # åªè¿”å›æˆåŠŸæå–åˆ°çš„å†…å®¹
            result = {"å®Œæ•´åˆ†æ": analysis}
            
            if core_value != "æœªæå–åˆ°":
                result["æ ¸å¿ƒä»·å€¼"] = core_value
            
            if target_users != "æœªæå–åˆ°":
                result["ç›®æ ‡ç”¨æˆ·"] = target_users
            
            if revenue_model != "æœªæå–åˆ°":
                result["æ”¶å…¥æ¨¡å¼"] = revenue_model
            
            if pitch != "æœªæå–åˆ°":
                result["ä¸€å¥è¯æ¨é”€"] = pitch
                
            return result
        
        except Exception as e:
            print(f"âŒ åˆ†æäº§å“æ—¶å‡ºé”™: {str(e)}")
            return {
                "å®Œæ•´åˆ†æ": f"åˆ†æå¤±è´¥: {str(e)}"
            }
    
    def extract_key_points(self, analysis):
        """ä»å®Œæ•´åˆ†æä¸­æå–å…³é”®ç‚¹"""
        # é»˜è®¤å€¼
        core_value = "æœªæå–åˆ°"
        target_users = "æœªæå–åˆ°"
        revenue_model = "æœªæå–åˆ°"
        pitch = "æœªæå–åˆ°"
        
        # å°è¯•æå–æ ¸å¿ƒä»·å€¼
        if "ğŸ’¡ è¿™ä¸ªäº§å“è§£å†³çš„æ˜¯ä»€ä¹ˆé—®é¢˜ï¼Ÿ" in analysis:
            parts = analysis.split("ğŸ’¡ è¿™ä¸ªäº§å“è§£å†³çš„æ˜¯ä»€ä¹ˆé—®é¢˜ï¼Ÿ")
            if len(parts) > 1:
                value_section = parts[1].split("\n\n")[0].strip()
                if value_section:
                    core_value = value_section[:100] + "..." if len(value_section) > 100 else value_section
        
        # å°è¯•æå–ç›®æ ‡ç”¨æˆ·
        if "ğŸ‘¤ ç”¨æˆ·æ˜¯è°ï¼Ÿ" in analysis:
            parts = analysis.split("ğŸ‘¤ ç”¨æˆ·æ˜¯è°ï¼Ÿ")
            if len(parts) > 1:
                user_section = parts[1].split("\n\n")[0].strip()
                if user_section:
                    target_users = user_section[:100] + "..." if len(user_section) > 100 else user_section
        
        # å°è¯•æå–æ”¶å…¥æ¨¡å¼
        if "ğŸ’° å®ƒèµšé’±å—ï¼Ÿå¤šå°‘ï¼Ÿ" in analysis:
            parts = analysis.split("ğŸ’° å®ƒèµšé’±å—ï¼Ÿå¤šå°‘ï¼Ÿ")
            if len(parts) > 1:
                revenue_section = parts[1].split("\n\n")[0].strip()
                if revenue_section:
                    revenue_model = revenue_section[:100] + "..." if len(revenue_section) > 100 else revenue_section
        
        # å°è¯•æå–ä¸€å¥è¯æ¨é”€
        if "ğŸ¤— ä¸€å¥è¯æ¨é”€ï¼š" in analysis:
            parts = analysis.split("ğŸ¤— ä¸€å¥è¯æ¨é”€ï¼š")
            if len(parts) > 1:
                pitch_section = parts[1].split("\n\n")[0].strip()
                if pitch_section:
                    pitch = pitch_section[:100] + "..." if len(pitch_section) > 100 else pitch_section
        
        return core_value, target_users, revenue_model, pitch
    
    def generate_mock_analysis(self, name, desc, website):
        """ç”Ÿæˆæ¨¡æ‹Ÿçš„åˆ†ææ•°æ®ï¼ˆå½“APIä¸å¯ç”¨æ—¶ä½¿ç”¨ï¼‰"""
        # ä½¿ç”¨ç®€çŸ­å ä½ç¬¦å†…å®¹æ¥æ›¿ä»£å®Œæ•´çš„åˆ†ææ¡†æ¶
        mock_responses = {
            "é—®é¢˜": f"{name}å¸®åŠ©ç”¨æˆ·è‡ªåŠ¨åŒ–å®Œæˆå¤æ‚çš„ä»»åŠ¡ï¼Œæé«˜å·¥ä½œæ•ˆç‡ï¼Œå‡å°‘é‡å¤åŠ³åŠ¨ã€‚",
            "ç”¨æˆ·": "ä¸»è¦é¢å‘ä¸“ä¸šäººå£«ã€åˆ›æ„å·¥ä½œè€…å’Œéœ€è¦æé«˜å·¥ä½œæ•ˆç‡çš„çŸ¥è¯†å·¥ä½œè€…ã€‚",
            "éœ€æ±‚": "ç”¨æˆ·éœ€è¦å®ƒæ¥èŠ‚çœæ—¶é—´ï¼Œæé«˜ç”Ÿäº§åŠ›ï¼Œå‡å°‘é‡å¤æ€§å·¥ä½œï¼Œè·å¾—æ›´é«˜è´¨é‡çš„è¾“å‡ºç»“æœã€‚",
            "è¯„ä»·": "å¤§å¤šæ•°ç”¨æˆ·å¯¹äº§å“çš„æ˜“ç”¨æ€§å’ŒåŠŸèƒ½å¼ºå¤§ç»™äºˆå¥½è¯„ï¼Œä½†ä¹Ÿæœ‰äººæåˆ°å¸Œæœ›æ”¹è¿›çš„æ–¹é¢ã€‚",
            "è·å®¢": "ä¸»è¦é€šè¿‡ç¤¾äº¤åª’ä½“è¥é”€ã€å†…å®¹è¥é”€å’Œå£ç¢‘ä¼ æ’­è·å–ç”¨æˆ·ã€‚",
            "ç›ˆåˆ©": "æ˜¯çš„ï¼Œé€šè¿‡è®¢é˜…æ¨¡å¼å’Œå¢å€¼æœåŠ¡å®ç°ç›ˆåˆ©ï¼Œå…·ä½“æ”¶å…¥æœªå…¬å¼€ã€‚",
            "å­¦åˆ°": "ä¼˜ç§€çš„äº§å“éœ€è¦åŒæ—¶å…³æ³¨ç”¨æˆ·ä½“éªŒå’Œè§£å†³å®é™…é—®é¢˜çš„èƒ½åŠ›ï¼Œäº§å“åŠŸèƒ½è¦èšç„¦æ ¸å¿ƒä»·å€¼ã€‚",
            "éš¾ç‚¹": "å»ºç«‹ç”¨æˆ·ä¿¡ä»»å’Œæ„å»ºå¤æ‚è€Œç›´è§‚çš„ç”¨æˆ·ç•Œé¢æ˜¯æœ€å…·æŒ‘æˆ˜æ€§çš„éƒ¨åˆ†ã€‚",
            "æ¨é”€": f"{name}è®©ä½ ä»¥åå€é€Ÿåº¦å®Œæˆå·¥ä½œï¼ŒåŒæ—¶æé«˜è¾“å‡ºè´¨é‡ã€‚",
            "æ–¹æ³•": "ä¸ç«å“ç›¸æ¯”ï¼Œè¯¥äº§å“æ›´æ³¨é‡ç”¨æˆ·ä½“éªŒå’Œè‡ªåŠ¨åŒ–ç¨‹åº¦ï¼Œè€Œéå•çº¯åŠŸèƒ½å †ç Œã€‚",
            "å¯è¡Œ": "æŠ€æœ¯ä¸Šå¯è¡Œï¼Œä½†éœ€è¦è¾ƒå¼ºçš„äº§å“è®¾è®¡èƒ½åŠ›å’Œå¯¹ç”¨æˆ·éœ€æ±‚çš„æ·±åˆ»ç†è§£ã€‚",
            "ç”¨æˆ·": "å¯ä»¥é€šè¿‡å†…å®¹è¥é”€ã€ç¤¾åŒºå»ºè®¾å’Œæä¾›å…è´¹è¯•ç”¨å¸å¼•åˆå§‹ç”¨æˆ·ã€‚",
            "ä¸ºä½•": "å¦‚æœä½ å¯¹è¿™ä¸ªé¢†åŸŸæœ‰çƒ­æƒ…å’Œç‹¬ç‰¹è§è§£ï¼Œé‚£ä¹ˆä½ æœ‰æœºä¼šå¼€å‘ç±»ä¼¼äº§å“ã€‚",
            "åšæŒ": "è¿™å–å†³äºä½ çš„çƒ­æƒ…å’Œå¯¹è§£å†³ç”¨æˆ·é—®é¢˜çš„æ‰¿è¯ºï¼Œä»¥åŠé•¿æœŸæŠ•å…¥çš„èƒ½åŠ›ã€‚"
        }
        
        # åˆ†ææ¡†æ¶çš„è¡Œ
        framework_lines = ANALYSIS_FRAMEWORK.strip().split('\n')
        
        # æ„å»ºå“åº”
        response_parts = []
        current_key = None
        
        for line in framework_lines:
            line = line.strip()
            if not line:
                continue
                
            # æ¡†æ¶çš„æ ‡é¢˜è¡Œ
            if line.startswith('##'):
                continue
                
            # æ£€æµ‹é—®é¢˜è¡Œ
            if any(emoji in line for emoji in ["ğŸ’¡", "ğŸ‘¤", "ğŸ¤”", "ğŸ—£ï¸", "ğŸ”", "ğŸ’°", "ğŸ§ ", "ğŸ¤—", "ğŸ‰", "ğŸ§­", "â¤ï¸"]):
                question = line
                response_parts.append(question)
                
                # ç¡®å®šå½“å‰é—®é¢˜çš„é”®
                if "è§£å†³çš„æ˜¯ä»€ä¹ˆé—®é¢˜" in line:
                    current_key = "é—®é¢˜"
                elif "ç”¨æˆ·æ˜¯è°" in line:
                    current_key = "ç”¨æˆ·"
                elif "ä¸ºä»€ä¹ˆéœ€è¦å®ƒ" in line:
                    current_key = "éœ€æ±‚"
                elif "å¦‚ä½•è¯„ä»·" in line:
                    current_key = "è¯„ä»·"
                elif "å¦‚ä½•æ‰¾åˆ°ç”¨æˆ·" in line:
                    current_key = "è·å®¢"
                elif "èµšé’±" in line:
                    current_key = "ç›ˆåˆ©"
                elif "å­¦åˆ°äº†ä»€ä¹ˆ" in line:
                    current_key = "å­¦åˆ°"
                elif "ä»€ä¹ˆåšæ³•ä¸å®¹æ˜“" in line:
                    current_key = "éš¾ç‚¹"
                elif "ä¸€å¥è¯æ¨é”€" in line:
                    current_key = "æ¨é”€"
                elif "ä¸åŒçš„æ–¹æ³•" in line:
                    current_key = "æ–¹æ³•"
                elif "èƒ½åšå‡ºæ¥" in line:
                    current_key = "å¯è¡Œ"
                elif "æ‰¾åˆ°ç”¨æˆ·" in line:
                    current_key = "ç”¨æˆ·"
                elif "ä¸ºä»€ä¹ˆæ˜¯æˆ‘" in line:
                    current_key = "ä¸ºä½•"
                elif "èƒ½åšæŒ" in line:
                    current_key = "åšæŒ"
                else:
                    current_key = None
                
                # æ·»åŠ å¯¹åº”çš„æ¨¡æ‹Ÿå›ç­”
                if current_key and current_key in mock_responses:
                    response_parts.append(mock_responses[current_key])
                else:
                    response_parts.append("æš‚æ— æ•°æ®ã€‚")
                
                # æ·»åŠ ç©ºè¡Œ
                response_parts.append("")
        
        # è¿æ¥æ‰€æœ‰éƒ¨åˆ†
        return "\n".join(response_parts)

    def process_excel_file(self, file_path, limit=None):
        """å¤„ç†å•ä¸ªExcelæ–‡ä»¶"""
        # åŠ è½½æ•°æ®
        df, is_chinese = self.load_excel(file_path)
        if df is None:
            return False
        
        # é™åˆ¶å¤„ç†çš„è¡Œæ•°ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if limit is not None and limit > 0:
            df = df.head(limit)
        
        # å‡†å¤‡æ–°çš„åˆ—
        df["å®Œæ•´åˆ†æ"] = ""
        
        # é€è¡Œå¤„ç†æ•°æ®
        print(f"ğŸ” å¼€å§‹åˆ†æäº§å“ (å…± {len(df)} ä¸ª)")
        
        # ä½¿ç”¨rangeå’Œæ•´æ•°ç´¢å¼•é¿å…linteré”™è¯¯
        for idx in range(len(df)):
            row = df.iloc[idx]
            product_data = row.to_dict()
            product_name = product_data.get("Tool Name", f"äº§å“ {idx+1}")
            
            print(f"\nğŸ“Š åˆ†æç¬¬ {idx+1}/{len(df)} ä¸ªäº§å“: {product_name}")
            
            # åˆ†æäº§å“
            analysis_results = self.analyze_product(product_data)
            
            # æ›´æ–°DataFrame - åªæ·»åŠ æœ‰å†…å®¹çš„åˆ—
            for key, value in analysis_results.items():
                if key not in df.columns and key != "å®Œæ•´åˆ†æ":
                    df[key] = ""
                if key == "å®Œæ•´åˆ†æ":
                    df.at[idx, key] = value
                elif key in df.columns:
                    df.at[idx, key] = value
            
            print(f"âœ… äº§å“åˆ†æå®Œæˆ: {product_name}")
            
            # æ¯å¤„ç†ä¸€ä¸ªæ‰¹æ¬¡åä¿å­˜è¿›åº¦
            if (idx + 1) % self.batch_size == 0 or idx == len(df) - 1:
                self.save_progress(df, file_path)
            
            # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…APIè¯·æ±‚è¿‡äºé¢‘ç¹
            if idx < len(df) - 1:
                print(f"â³ ç­‰å¾… {self.delay} ç§’...")
                time.sleep(self.delay)
        
        # ç§»é™¤æ‰€æœ‰å€¼ä¸ºç©ºå­—ç¬¦ä¸²çš„åˆ—
        for col in df.columns:
            if col not in ["Ranking", "Tool Name", "Tool Link", "Website", "Payment Platform", "Monthly Visits", "Description", "Revenue"]:
                # æ£€æŸ¥åˆ—æ˜¯å¦å…¨ä¸ºç©ºæˆ–"æœªæå–åˆ°"
                if df[col].astype(str).str.strip().isin(["", "æœªæå–åˆ°"]).all():
                    df = df.drop(columns=[col])
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        output_path = self.save_final_result(df, file_path)
        
        print(f"\nâœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {os.path.basename(file_path)}")
        print(f"âœ… ç»“æœå·²ä¿å­˜è‡³: {output_path}")
        
        return output_path
    
    def save_progress(self, df, original_file_path):
        """ä¿å­˜å¤„ç†è¿›åº¦"""
        try:
            # ç”Ÿæˆä¸´æ—¶æ–‡ä»¶å
            basename = os.path.basename(original_file_path)
            filename, ext = os.path.splitext(basename)
            progress_path = os.path.join(self.output_dir, f"{filename}_progress{ext}")
            
            # ä¿å­˜DataFrame
            df.to_excel(progress_path, index=False)
            
            print(f"âœ… è¿›åº¦å·²ä¿å­˜è‡³: {progress_path}")
            return progress_path
        
        except Exception as e:
            print(f"âŒ ä¿å­˜è¿›åº¦å¤±è´¥: {str(e)}")
            return None
    
    def save_final_result(self, df, original_file_path):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        try:
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            basename = os.path.basename(original_file_path)
            filename, ext = os.path.splitext(basename)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = os.path.join(self.output_dir, f"{filename}_analyzed_{timestamp}{ext}")
            
            # ä¿å­˜DataFrame
            df.to_excel(output_path, index=False)
            
            return output_path
        
        except Exception as e:
            print(f"âŒ ä¿å­˜æœ€ç»ˆç»“æœå¤±è´¥: {str(e)}")
            return None
    
    def run(self, limit=None, language=None):
        """è¿è¡Œå¤„ç†æµç¨‹"""
        # æŸ¥æ‰¾Excelæ–‡ä»¶
        if language == "cn":
            pattern = os.path.join(self.data_dir, "Toolify_Top_AI_Revenue_Rankings_CN_*.xlsx")
        elif language == "en":
            pattern = os.path.join(self.data_dir, "Toolify_Top_AI_Revenue_Rankings_EN_*.xlsx")
        else:
            pattern = os.path.join(self.data_dir, "Toolify_Top_AI_Revenue_Rankings_*.xlsx")
        
        excel_files = self.find_excel_files(pattern)
        
        if not excel_files:
            return False
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        results = []
        for file_path in excel_files:
            output_path = self.process_excel_file(file_path, limit)
            if output_path:
                results.append(output_path)
        
        if results:
            print(f"\nâœ… å…¨éƒ¨å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(results)} ä¸ªæ–‡ä»¶:")
            for path in results:
                print(f"   - {path}")
            return True
        else:
            print("\nâŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ–‡ä»¶")
            return False

def main():
    import argparse
    
    # è·å–Dç›˜ä¸‹çš„å·¥ä½œç›®å½•
    d_drive_path = "D:\\code\\chromedriver-win64"
    if os.path.exists(d_drive_path):
        os.chdir(d_drive_path)
        print(f"âœ… å·²åˆ‡æ¢åˆ°å·¥ä½œç›®å½•: {d_drive_path}")
    
    parser = argparse.ArgumentParser(description="Toolifyæ•°æ®åˆ†æé›†æˆå·¥å…·")
    parser.add_argument("-d", "--data-dir", default="./toolify_data", help="æ•°æ®ç›®å½•è·¯å¾„")
    parser.add_argument("-o", "--output-dir", default="./output", help="è¾“å‡ºç›®å½•è·¯å¾„")
    parser.add_argument("-b", "--batch-size", type=int, default=5, help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("-l", "--limit", type=int, help="æ¯ä¸ªæ–‡ä»¶å¤„ç†çš„æœ€å¤§è®°å½•æ•°")
    parser.add_argument("-t", "--timeout", type=int, default=120, help="APIè°ƒç”¨è¶…æ—¶æ—¶é—´(ç§’)")
    parser.add_argument("-w", "--wait", type=int, default=2, help="APIè°ƒç”¨é—´å»¶è¿Ÿ(ç§’)")
    parser.add_argument("--language", choices=["cn", "en", "all"], default="all", help="å¤„ç†çš„è¯­è¨€ç‰ˆæœ¬")
    
    args = parser.parse_args()
    
    language = None if args.language == "all" else args.language
    
    integrator = ProductAnalysisIntegrator(
        data_dir=args.data_dir,
        output_dir=args.output_dir,
        batch_size=args.batch_size,
        timeout=args.timeout,
        delay=args.wait
    )
    
    integrator.run(limit=args.limit, language=language)

if __name__ == "__main__":
    main() 